{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import skipthoughts\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Corpus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(text): \n",
    "    remove = string.punctuation\n",
    "    remove = re.sub(\"[\\,\\-\\%']\", \"\", remove) # don't remove hyphens, commas and apostrophes\n",
    "    \n",
    "    pattern_end_sentence = r\"[{}]$\".format(remove)\n",
    "    text = re.sub(pattern_end_sentence, \"\", text)\n",
    "    \n",
    "    pattern = r\"[{}]\\s?\".format(remove)\n",
    "    return re.split(pattern, text)\n",
    "\n",
    "def remove_punct(sentences): \n",
    "    remove = string.punctuation\n",
    "    remove = re.sub(\"[\\']\", \"\", remove) # don't remove apostrophes\n",
    "    \n",
    "    pattern_punct = r\"[{}]\".format(remove)\n",
    "    pattern_apoth = r\"^\\'\"\n",
    "    pattern_first_space = r\"^\\s\"\n",
    "\n",
    "    return list(\n",
    "        filter(lambda sent : len(sent) != 0,\n",
    "                       list(\n",
    "        map(lambda sentence : \n",
    "            \"\".join(\n",
    "                re.split(pattern_punct, \n",
    "                         re.sub(pattern_first_space, \n",
    "                                \"\", \n",
    "                                re.sub(pattern_apoth, \"\", sentence))\n",
    "                        )\n",
    "            ), sentences)\n",
    "    )\n",
    "              )\n",
    "    )\n",
    "\n",
    "\n",
    "def data_processing(df_text_corpus, col_list_text):\n",
    "    texts_tokenized = df_text_corpus[col_list_text].map(lambda text : sentence_tokenizer(text))\n",
    "    texts_tokenized_cleaned = texts_tokenized.map(lambda text : remove_punct(text))\n",
    "    \n",
    "    return texts_tokenized_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipthought_encode(list_text):\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the list_text\n",
    "    \"\"\"\n",
    "    enc_list_text = [None]*len(list_text)\n",
    "    cum_sum_sentences = [0]\n",
    "    sent_count = 0\n",
    "    for text in list_text:\n",
    "        sent_count += len(text)\n",
    "        cum_sum_sentences.append(sent_count)\n",
    "\n",
    "    all_sentences = [sent for text in list_text for sent in text]\n",
    "    print('Loading pre-trained models...')\n",
    "    model = skipthoughts.load_model()\n",
    "    encoder = skipthoughts.Encoder(model)\n",
    "    print('Encoding sentences...')\n",
    "    enc_sentences = encoder.encode(all_sentences, verbose=False)\n",
    "\n",
    "    for i in range(len(list_text)):\n",
    "        begin = cum_sum_sentences[i]\n",
    "        end = cum_sum_sentences[i+1]\n",
    "        enc_list_text[i] = enc_sentences[begin:end]\n",
    "    return enc_list_text\n",
    "        \n",
    "    \n",
    "def summarize(list_text, original_list_text):\n",
    "    \"\"\"\n",
    "    Performs summarization of list_text\n",
    "    \"\"\"\n",
    "    nb_text = len(list_text)\n",
    "    summary = [None]*nb_text\n",
    "    \n",
    "    print('Starting to encode...')\n",
    "    enc_list_text = skipthought_encode(list_text)\n",
    "    \n",
    "    print('Encoding Finished')\n",
    "    \n",
    "    for i in range(nb_text):\n",
    "        enc_text = enc_list_text[i]\n",
    "        n_clusters = int(np.ceil(len(enc_text)**0.5))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        kmeans = kmeans.fit(enc_text)\n",
    "        avg = []\n",
    "        closest = []\n",
    "        for j in range(n_clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n",
    "                                                   enc_text)\n",
    "        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "        \n",
    "        idx_sentences_choosed = sorted([closest[idx] for idx in ordering])\n",
    "        \n",
    "        summary[i] = '. '.join([original_list_text[i][idx] for idx in idx_sentences_choosed])\n",
    "        \n",
    "    print('Clustering Finished')\n",
    "    \n",
    "    return summary\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was a failure by the US men’s national ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text\n",
       "0  This was a failure by the US men’s national ba..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = [\"\"\"This was a failure by the US men’s national basketball team, no question about it. Team USA don’t suffer too many defeats, so each one tends to feel like a disaster. To be more specific, Wednesday’s 89-78 loss to France in the quarter-finals of the Fiba World Cup was the program’s first since 2006 (Thursday’s loss to Serbia guaranteed USA its worst placing in a major international tournament). While the France defeat came earlier in the tournament than many had predicted, nobody can claim it was completely unexpected. History, however, tells us that the Americans will be back and stronger than ever. In the meantime, let these results be another example of the health of the sport around the world.\"\"\"]\n",
    "text_corpus = pd.DataFrame({\"article_text\": text_test})\n",
    "text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list_text = [text.split('. ') for text in text_corpus['article_text'].values]\n",
    "texts_processed = data_processing(text_corpus, \"article_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text_processed = texts_processed.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to encode...\n",
      "Loading pre-trained models...\n",
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n",
      "Encoding sentences...\n",
      "Encoding Finished\n",
      "Clustering Finished\n"
     ]
    }
   ],
   "source": [
    "summaries = summarize(list_text_processed, original_list_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was a failure by the US men’s national basketball team, no question about it. History, however, tells us that the Americans will be back and stronger than ever. In the meantime, let these results be another example of the health of the sport around the world.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was a failure by the US men’s national basketball team, no question about it. Team USA don’t suffer too many defeats, so each one tends to feel like a disaster. To be more specific, Wednesday’s 89-78 loss to France in the quarter-finals of the Fiba World Cup was the program’s first since 2006 (Thursday’s loss to Serbia guaranteed USA its worst placing in a major international tournament). While the France defeat came earlier in the tournament than many had predicted, nobody can claim it was completely unexpected. History, however, tells us that the Americans will be back and stronger than ever. In the meantime, let these results be another example of the health of the sport around the world.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus[\"article_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
